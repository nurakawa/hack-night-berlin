{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AssemblyAI Challenge\n",
    "by Nura Kawa\n",
    "\n",
    "Instructions for the challenge: https://docs.google.com/document/d/14nddyffRNvMIu4PDRy3oPqMMPYi-dkW_a8-c0RXQaXk/edit?tab=t.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, run the line below\n",
    "# !pip install assemblyai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Transcribe an audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import assemblyai as aai\n",
    "\n",
    "aai.settings.api_key = \"\" # place your API key here\n",
    "\n",
    "config = aai.TranscriptionConfig(\n",
    "    speaker_labels=True,\n",
    "    speakers_expected=2,\n",
    "    sentiment_analysis=True)\n",
    "\n",
    "transcriber = aai.Transcriber()\n",
    "transcript = transcriber.transcribe(\n",
    "  \"https://assembly.ai/weaviate-podcast-109.mp3\",\n",
    "  config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and print the number of different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transcript has 362 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences = transcript.get_sentences()\n",
    "n_sentences = len(sentences)\n",
    "\n",
    "print(f'The transcript has {n_sentences} sentences.')\n",
    "\n",
    "# Note: I have gotten results in the range [361, 364]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Identify Speakers and Analyze Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Identify speakers in the podcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Hey everyone, thank you so much for watching another episode of the weva podcast. I'm super excited to welcome Erica Cardenas. Erica is a technology partner manager at weaviate, where she leads the effort behind weaviate recipes and all sorts of things managing weaviate's partnerships with partners such as Google Cohere, Modal and many others. Eric and I, as well as other members from the Weaviate team, will be in New York City next Tuesday, November 19th at the Google Pier 57 building. We'd love to meet you there, so if interested in that, please see the link in the description. Erica, thanks so much for joining the WEV8 podcast.\n",
      "B: Awesome. Thank you for having me. I'm excited to talk about Agentic Rag and hopefully meet a few people at the event in New York.\n",
      "A: Awesome. So for our listeners who aren't aware, last week Eric and Leigh and I published a new blog post on weaviate's blogs, Agentic Rag. And so in this podcast we really want to just keep diving into Agentic Rag, explain why we think it's a why now moment for weaviate, the benefits of Agentic Rag and all these things. So, Erica, even before we kick this off, could you maybe start us from the beginning? What are agents?\n",
      "B: Yeah, awesome. So I'll cover. I think it's important to first cover the components of an agent. So of course you have the language model, but then you also have the memory, whether it be short term or long term. And we've seen new frameworks like Letta come out of this with just updating its memory based off of conversation history. But then you also have planning.\n",
      "A: So.\n"
     ]
    }
   ],
   "source": [
    "for utterance in transcript.utterances[0:5]:\n",
    "    print(f'{utterance.speaker}: {utterance.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am not successful in extracting Speaker names. Luckily, we can ask an LLM to help us!\n",
    "\n",
    "speaker_prompt = \"What are the names of the two speakers in this podcast?\"\n",
    "speaker_prompt_result = transcript.lemur.task(\n",
    "    speaker_prompt, final_model=aai.LemurModel.claude3_5_sonnet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two speakers in this podcast are:\n",
      "\n",
      "1. Connor (the host)\n",
      "2. Erica Cardenas\n",
      "\n",
      "Connor introduces Erica at the beginning of the podcast as a technology partner manager at Weaviate. They have a conversation throughout the podcast about topics related to AI agents, RAG (Retrieval-Augmented Generation), and generative feedback loops.\n"
     ]
    }
   ],
   "source": [
    "print(speaker_prompt_result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_name = lambda x: 'Speaker A (Connor)' if x == 'A' else 'Speaker B (Erica)'    \n",
    "\n",
    "for utterance in transcript.utterances:\n",
    "    utterance.speaker = change_name(utterance.speaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print the transcript by speaker utterances:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker A (Connor): Hey everyone, thank you so much for watching another episode of the weva podcast. I'm super excited to welcome Erica Cardenas. Erica is a technology partner manager at weaviate, where she leads the effort behind weaviate recipes and all sorts of things managing weaviate's partnerships with partners such as Google Cohere, Modal and many others. Eric and I, as well as other members from the Weaviate team, will be in New York City next Tuesday, November 19th at the Google Pier 57 building. We'd love to meet you there, so if interested in that, please see the link in the description. Erica, thanks so much for joining the WEV8 podcast.\n",
      "Speaker B (Erica): Awesome. Thank you for having me. I'm excited to talk about Agentic Rag and hopefully meet a few people at the event in New York.\n",
      "Speaker A (Connor): Awesome. So for our listeners who aren't aware, last week Eric and Leigh and I published a new blog post on weaviate's blogs, Agentic Rag. And so in this podcast we really want to just keep diving into Agentic Rag, explain why we think it's a why now moment for weaviate, the benefits of Agentic Rag and all these things. So, Erica, even before we kick this off, could you maybe start us from the beginning? What are agents?\n",
      "Speaker B (Erica): Yeah, awesome. So I'll cover. I think it's important to first cover the components of an agent. So of course you have the language model, but then you also have the memory, whether it be short term or long term. And we've seen new frameworks like Letta come out of this with just updating its memory based off of conversation history. But then you also have planning.\n",
      "Speaker A (Connor): So.\n",
      "Speaker B (Erica): So you can have like chain of thought or sub question. I want to say sub question query engine because of Llama, but it can break down a question into sub questions before passing it on to the tool. So to query your database or do a web search, anything really, or even react, obviously is another very popular one. And then of course you have tools, whether that be maybe to your database or the web search, like I said, or like the Slack API calculator, all of it. Anything that has an API, these language models can access through function calling. So yeah, that's kind of like what makes up an agent, I would say.\n",
      "Speaker A (Connor): Yeah, awesome. I use the same definition of this function calling loop thing where it's just calling a function, seeing the results from the function call, and then saying, hey, have I finished the task or am I going to call more functions and continue this looping process? And I think it's really interesting Just this general distinction between that open ended looping process or hard coding some particular flow of prompts and tool use in a pipeline. I like to call those compound AI systems, although I think you kind of could call an agent a compound AI system and vice versa. But this, I think these are the two kind of camps that are emerging with designing these systems. So could we. Now with this kind of function calling paradigm, could we describe how agentic rag differs from vanilla rag?\n",
      "Speaker B (Erica): Yeah, so agents differ from vanilla rag because with vanilla rag you just have the retrieve augment and then generate. It's like a very standard pipeline. So if I ask what is hybrid search? It will grab my most relevant blog chunks on hybrid search and then it will send it to the language model and it will generate a response. But with agents or agentic rag, it's able to plan the required steps needed to achieve the user's query. It can of course do the sub question. It can do chain of thought reasoning before accessing those specific tools. Another cool thing is with the function calling loop that you just referred to, it can determine if it needs more information by retrieving from other sources. I of course have my weaviate blog trunk stored in my Web8 database, but maybe there is some new research or something about hybrid search that is only on the web and not in my database quite yet. It's able to say, oh, I actually don't have enough context. Let me go to the web and you can maybe even have like a bigquery database as well. Anyway, yeah, probably get carried away if I keep going down that path. But then of course it can call more than one tool and then summarize the results if needed. So you can go to your database, you can go to your web search, maybe even a lack conversation history. If of course you set that up as a tool. It's not done automatically. People don't have to fear about their conversations. But I'd say the autonomy of these agents to be able to query different databases and just sources of information is what makes it different from vanilla rag.\n",
      "Speaker A (Connor): Yeah, I already want to take the beta, start the generative feedback loop, talking about how exciting this is going to be to be able to connect other data sources to weaviate through generative feedback loops and just flow your data through weaviate or whatever you're doing. But we'll save that one for the ending. For now we're just continuing to dive into agentic rag and why it's so new. We have these benefits like you mentioned about the writing the Queries, being able to execute queries in parallel and then being able to iteratively search and then navigate, navigate different indexes as well as use filters and kind of graduate from just the search query to being able to just get objects into the database. And so I think kind of the, the function calling thing that I'm still a little unclear about is this idea of planning. I'm still very unclear to me what it means by, you know, make a plan. So, yeah, I love to just unpack this. What do you think the role of planning is in these agent systems?\n",
      "Speaker B (Erica): Yeah, awesome. Well, I think one benefit that I didn't touch on and you actually just covered is being able to navigate your database to run vector, search aggregate or filter quer. So with Vanilla Rag, if you are doing a semantic search query and you haven't hard coded a filter, you know, you're like, okay, one query can be pull my conversation history with Connor from January 10, 2024. If I were to run a semantic search query, the only thing really that would be able to retrieve is just our conversation history between us. Unless of course, the date is in the content of our message. But with Agentic Rag, it can take the user's query, say, okay, this requires a semantic search, but then it can also add some metadata filters to the query and then pass on. Call the pass on the query to the collections in that kind of sense. I think that's kind of like an interesting benefit of it. And you know, in order to take the metadata property and add it to your query, a chain of thought or even a react kind of planning capability is necessary because it needs to kind of think and reason about the user's question before it can properly call the function and then run that loop in order to answer the query in the best way possible.\n",
      "Speaker A (Connor): Yeah, I think I'm just, I guess I'm really curious about how. So the react. React is this sequential architecture of you always have some meta assess the state of the, of the task prompt that it does in between calling calling functions. And so I'm kind of, I'm really curious how that differs from chain of thought. If you think that these are, these really are different ideas, like, is this really a different idea? Because I, the way that I've been implementing React with function calling is I just have like thought internal state of mind is one of the required arguments. And so it kind of does like a rationale for its tool call in that sort of interface. And so I guess, yeah. Are we thinking that React and Chain of Thought are Pretty similar, yeah.\n",
      "Speaker B (Erica): Interesting. I'd say no, obviously, because they're two separate things, at least how I've seen it on online. So you'd say with chain of thought it's like the initial prompt to like dissect the user question. Whereas with React it's kind of more iterative and it has a loop so it will reason about its current state and then it will act in a certain way. I guess it's kind of just like a forced loop. So because it's able to act after it's like reasoned about the question, I would say it's different from chain of thought. The exact implementation of it. Not super familiar, but I would say it's like just two separate ways to do that. And I think one like thing that people have seen is with GPT4, GPT401 and how it's breaking down the user question. It's kind of like spitting out what it's doing under the hood and then it's writing a whole bunch of text or generating an image and then it's able to like go back to the beginning and make sure that it stayed on track. I would say that's more of like the REACT framework and also the inventor of it is that OpenAI. So I find it likely that it's REACT under the hood for O1, but could be a theory. I don't know.\n",
      "Speaker A (Connor): Yeah, I think that comes into that. Kind of like streaming interfaces for agents and how important it is. Or like agents compound AI systems. I think the abstraction here is still like up in the air, but anytime you're going to be doing this, like, you know, it's, it's going to be pretty latency heavy and you want to send something to the front end about hey, it's still alive back here. You don't want it to just be like spinning circle. Yeah. So that inspired me to thinking about O1 and I think the chain of thought in this. I'm still kind of trying to make sense of this planning idea and they, they have this kind of like tree of thoughts algorithm. There's this thing with the mu0 algorithm that had, when they had beat the world champion go player with AI, it was about doing this like Monte Carlo tree search where you'd sort of roll out the tree, you'd have a thought and then you'd say, okay, if I do take this action, it'll take me to this state of the world. And then I could. And then from there I could do these actions and that might take me to these different States and so you kind of like unroll that tree all the way forward. And so I'm curious if you think that kind of thing could be like useful in this agent framework where you make this plan where you say, okay, I could search blogs and that might get me to this kind of information or I could search in my analytics table or something like that. Like so I'm still trying to make sense of what the analogy might be with this tree reasoning and then this kind of like agentic rag system. Hmm.\n",
      "Speaker B (Erica): Well, when did that come out? When was that released? I can't remember. It was a while ago. Right.\n",
      "Speaker A (Connor): I think the mu0 thing was quite a while ago. And then I think tree of thoughts. I see Tree of thought, it's Googled it. I see it's at 2023. Also Shen Yao from the react, Shen Yu from the REACT paper also did that one like a side project.\n",
      "Speaker B (Erica): I think I would say with tree of thought it kind of sounds more resource heavy. I would say in the initial stage, like it requires creating like a massive sense of self and like kind of like the different rabbit holes that you can go to or enter. Whereas with react, I feel like it's kind of like quick on its feet, if you would say, because it's able to like reason, act and then it's like, oh no, that wasn't right. Let me reason again and then act. I feel like that's kind of faster than projecting kind of the what ifs in that sense.\n",
      "Speaker A (Connor): Yeah, but yeah, yeah, it's really, I guess like taking like the reinforcement learning setting. You have this loop where it's like the agent and then the environment and at each step the agent can send a action and then it receives the next state in a potential reward. And so I think that's with our function calls. Like each function call is like an action and then you're taken to the next state of the results of the function call. And so from there you can do all that like model based reinforcement learning planning stuff. I don't know. But yeah, so it seems like something interesting, this planning thing. I don't, I don't know if we're going to be doing too much planning in our agent rags for the time being, but it's definitely kind of an interesting topic. So I think maybe more practically the next topic would be this multi agent systems we're seeing. Crewai, you looked into OpenAI Swarm and you did this research about the. I think it's like a customer support agent. Yeah. Could you tell us more about. Yeah, Multi agent and these kind of systems.\n",
      "Speaker B (Erica): Yeah, of course. So you have the single agent and then multi agent. So with single agent you just have one language model that is making the calls to the different functions and then it'll send it to the next language model to kind of like summarize the response if needed. But with a multi agent system you can have the top level agent and then you can have sub level agents that are each specialized in like a niche kind of category. So you can have like your webiate collection A and then you're also your rebate collection B. Like it's like two separate topics but then you can also have like the web search and then another tool I guess and the top level agent will call the necessary ones in parallel and each sub level agent can have its own resources. And I think that's like kind of similar to the thinking or like the design of with DSPY of how like you're breaking down the programs into like the signatures and you're like this is all you're going to do and this is all you're going to focus on. I think that's really cool. And one other thing with the multi agent kind of design is you can have different language models for each. And why I think that is so cool is because if you just have your weaviate collection maybe you could just use like a pretty small language model but let's say it's like a more open ended agent. So maybe like that top level one or even the final language model it can be like a powerful model that you would want to assign to that because it's like more broad. It might need to have like just more intelligence I guess. So that's like kind of the multi agent design is just having different language models, each with their own tools and resources.\n",
      "Speaker A (Connor): Awesome. Yeah, I think there are so many interesting nuggets in that. Yeah, it definitely seems like that's the dominant architecture of the multi agent thing is like GPT 4. 0 or 01 is the, you know, the controlling the orchestrator node and then you've got say llama. Llama 8B's is the, they do some particular role role I guess is the multi agent way of thinking about it. And I really like how you bring up dspy. Like you could think of the role being a prompt and you kind of learn to, you know, you tweak that prompt to have the LLM perfectly embody that role that it's supposed to serve. And then I had so many ideas. I, I like the you mentioned, letta earlier I think we have to talk talk more about Letta in the multi agent framework because I think it's so interesting this Letta is this framework based on memgbt for people who don't know that does this like meta memory thing.\n",
      "Speaker B (Erica): Yeah. So let updates the conversation history or it updates its memory based off of conversation history with the user. And why that is interesting is because let's say I've introduced myself to this chatbot saying that my name is Erica Guardinez and I'm a developer advocate at webiate. Now the next time I introduce myself I'm going to say hi, I'm Erica Codenas and I am a technology partner manager. Now it's going to update its memory with my new job title and it's going to keep that. It's going to, it's like a stateful agent because it's able to update its memory based off of conversation history.\n",
      "Speaker A (Connor): Yeah. And that I think that idea of having the short term long term memory separation where short term memory memory is something that's always in the prompt and it's so critical to this function calling loop thing because as it's calling functions and seeing the results it needs to be have this, you know, memory right in the prompt of the results of these functions. So adding this kind of meta thing. But yeah, it's super interesting this short term, long term memory distinction in the agent designs. But I really, I kind of really want to go further into how the Letta approach maybe differs from the DSPY approach. I think they're quite similar. They're both kind of like you're learning from data to optimize either the prompts sort of in this DSPY where you kind of have this like training test set. But with Letta and I see these multi agent systems, if I have a multi agent system where I have like the CEO agent and then I have like a team of marketers and a team of engineers or whatever and they start off with some initial role and it's like as we're doing tasks let can just like update the internal memory of each of the agents and they kind of like learn how to play their role through their experience and through the Letta framework. Yes. Maybe. How do you think about that kind of like the way that Letta thinks about evolving this multi agent system.\n",
      "Speaker B (Erica): Can I actually go back and ask you a question? Is that allowed?\n",
      "Speaker A (Connor): Yeah, it's allowed.\n",
      "Speaker B (Erica): Okay. For the memory component. Would you. Do you think it makes more sense to have the vector database as the long term memory or as a tool.\n",
      "Speaker A (Connor): Well, I think, I think the interesting thing here is like I guess the agents. Yeah, yeah, it's a great question. And then you have agentic rag on the agent's internal memory. That's probably the best way to do it, I guess at the agent, it's like you think about what kind of metadata does it have? Like if it receives a message, it's always just kind of receiving messages and sending responses. Right. So that's kind of like the history it's tracking. So if it's. If it's got this like metadata on, you know, last heroes, my messages from last week compared to two weeks ago, it can probably use that to. And maybe it also keeps a memory of the, of the internal state. You're actually reminding me that we have more work to do on our actual integration with Leto with weaviate. But I look forward to learning more about those kind of details and how let is thinking about architecting that kind of thing. I think they have a lot of opportunity to innovate on that kind of thing. So. Yeah, so maybe let's talk about the state of evaluation. Kind of like how do you generally feel like the AI market is sort of shaping out with evaluation?\n",
      "Speaker B (Erica): Yeah, a lot of companies have with observability and evals, they've raised a lot of money. So I think there's obviously a lot of research going into them. Of course you have Arises, Telemetry, Phoenix. It's pretty cool to be able to see the calls that are being made to these. The call to track trace. Trace the calls that are being sent to the language model. Because like you said earlier, I do think when you have built an agent system, it's really important to see what's going on underneath the hood because it's not quick. So what these tools are offering is being able to kind of track and see what's happening under the hood. And I had attended one talk from a user experience woman who worked at Google and how they're kind of bringing this into Vertex AI and it's like they have built their agent builder has like the prompts that you're defining. But then maybe if it's like breaking down a question, it will, it will tell you that I think like having the open communication with these agent systems is really important because there's a lot going on underneath the hood. So having tools and just like, yeah, just these different tools are really important for building these systems because yeah, people are probably getting very impatient quickly. Our attention span is very short.\n",
      "Speaker A (Connor): Yeah, I think the open telemetry thing has certainly had a massive impact in working with all these frameworks and I think this will be one of the biggest use cases of generative feedback loops is we started on this thing about make it stateful and you always want to save the outputs. I think basically the thing here is anytime you have LLM inference you want to save it probably in your database and see it later and run these kind of evaluations and all these kind of things. So yeah, I'd really love to transition into, well, I guess maybe quickly on that vertex agent builder nugget. I love when you showed me that it had that kind of like I think earlier today we were talking about the function calling JSON that you have to send to define each of the tools and it has this really kind of ugly thing of like a tool, a function, a parameter, a parameter property. What do you think about the kind of UX for defining tools and agents? That seems like a huge developer experience problem right now.\n",
      "Speaker B (Erica): Yeah, I think yeah you have the standard JSON currently, but I do think a low code or no code solution for building agents will be a huge thing especially for like broadening the reach. I think you know like language models aren't, weren't new but ChatGPT kind of like made it where everyone can like kind of understand what it is. I think like having the front end for and like an easy way to build these kind of systems will be very important. And yeah, I think Google is doing pretty good research in that.\n",
      "Speaker A (Connor): But yeah, yeah, that's such a cool one. I think that. And maybe there's something to. Well yeah, like where you host the tool, like where the tool actually runs. So like hopefully it just kind of you have your inference and your tool sort of like neatly packaged up in one runtime. I don't even know what to call it but I think those are like two of the big developer experience problems with agents which kind of coming back into evaluation. Observability. I'd love to talk more. We talked about observability and yeah, we love the visualization from Arise AI and Phoenix, this tracing tool and how it lets you see the call, see the traces when you wrap them into like agent or compound AI system structures and just keep it all in the same place. I love to talk about LLM as judge and evaluations. I know you love to talk about how, you know how silly it is that you could run the same inference five times and get five different results. So what do you think about LLM is judge and Sort of how that's evolving.\n",
      "Speaker B (Erica): I don't think using the language model solely is as reliable as people think, obviously with having like the temperature set or just not set. I've never seen people like set the temperature in their applications, kind of just like whatever the default is. So that's probably like one problem with using the LLMs as judge. And I remember on the Nils Reimers podcast he had talked about, you know, you can't really evaluate a system that is using the Cohere models and then use the OpenAI model as a judge because it's gonna have like biases obviously.\n",
      "Speaker A (Connor): So I think there needs. I think that idea is crazy though. How does cohere know that the response. I guess that's how subtle the patterns are. That reminds me of. I saw this meme on Twitter and it's like how it feels to be a latent vector in a transformer. And it's kind of like, you know, he's like, why? It's like a gravity turned off. It's like flying upside down. But the idea that GPT4 could tell that the responses from GPT4.0, that's pretty crazy. 1. And I love that nugget about the ensembling. Yeah. So what do you think? What? What do you think, you know, continuing on this like ensembling inference thing. I remember earlier we had looked into that paper. Are more agents all you need? That's saying at each step, just sample like 25 generations of the same prompt. Yeah. Do you think that's. What do you think about that?\n",
      "Speaker B (Erica): Well, I think sampling from the language model, maybe like 20 is a secret spot. I don't really know because. Yeah, like you said, with the more agents is all you need. I guess there was to a certain point where you don't need more agents. There was a rebuttal to it.\n",
      "Speaker A (Connor): Yeah. So I think, I think that that's perfect nugget for LM as judge is to run the inference 20 times and. Yeah. And get a more robust result that way. But I guess the interesting thing about that paper is that they show that just sample it many times. They separate between easy and hard with like, I think it's chemistry questions or physics questions. And so they're able to say that that doesn't work for the hard questions, but it does work for the easy questions, which I think is, you know, another one of those mind bending, who knows how that actually works type of things. Yeah. Awesome. So I think that was a pretty good roundup of agents. We've got all these topics around function calling and you know we've got the user experience for how you define your tools. This idea of the function calling loop, we've got ideas like using the DSPY avatar optimizer to optimize the description of the tool and then we've got all these things like what planning might look like, what multi agent systems might look like and this letta idea, we really think this is interesting, this short term memory idea and then of course how observability and evaluation changes. So now kind of an anchoring topic that we're really excited about is sort of how we VA is leveraging agent agentic systems with generative feedback loops. Eric, could you kick us off by describing how you see the role of agents in generative feedback loops?\n",
      "Speaker B (Erica): Sure, yeah. So obviously generative feedback loops is something near and dear to our hearth at weaviate and why agents play a role in gfls is because it's able to maybe generate new synthetic data, it's able to categorize your data maybe for model training there's like a variety of applications where GFLs play a role and kind of having like an agent system under the hood I think gives it a lot of autonomy and also just the results are better in my opinion other than like rather than just like doing rag to you know generate new data based off of the context. I do think that having it being able to you know, maybe do a web search but then also see your rebate collection kind of like this loop process is interesting for GFL specifically.\n",
      "Speaker A (Connor): Well yeah, I'd love to. Another thing we had looked into was this storm system from Stanford and now we've seen Obviously I think OpenAI's 01 is what's going to push the public adoption of these long running processes that produce some kind of thing And I can't wait for the same kind of system of like a mid journey 01 where I say I want my robot we v8 gorilla visual and it is like all right, I'll see you in the morning. I'll be working on this all night and I'll have a, you know a bunch of these pictures to show you in the morning. Yeah, can you maybe talk more about how you see this like yeah like long running process for generative feedback loops to you know create stuff with AI?\n",
      "Speaker B (Erica): Yeah, I mean I don't think, I think yeah the latency is obviously like a huge concern but I'd say yeah if you want your webiate branded gorilla with like looking super techno and like these specific details yeah, that's not going to take a minute. Maybe give it like a few hours to generate it. And similarly to Storm, you know, it's doing that thorough research before generating the report on a topic. Or even like our example of building a DSPY program to generate a new blog post from my query, you know, it was like write an outline. What was it? Oh, it was research, write an outline, generate the content and then now give, give it a title. It's like a four layer DSPY program. You know that it wasn't fast but it was a pretty good blog post. And I think that's like the difference in these kind of systems compared to like a chatbot where it can be just be like, you wanna know what hybrid search is?\n",
      "Speaker A (Connor): Boom.\n",
      "Speaker B (Erica): This is my answer. These are kind of like, you know, you really have to think about what you need to build. Then you'd access the tools to build it and then you need to make sure that it's right and then like kind of like have that whole flow going.\n",
      "Speaker A (Connor): Yeah, yeah, epic. And I guess for me the external tools thing, I remember I was working with Charles on the Recommend IT recommender system and it was like we had this data set called X wines. Like wine data sets, got features like the abbreviate, you know, the alcohol content, like what type of wine it is, where it comes from and stuff like that. And I wanted to add you know like a description of the wine just so I could benchmark this kind of like feature representation learning versus the text embeddings. And so that's when I first kind of connected it to the web search to add this data about the wines. Like could you do the web search to describe like what is Chardonnay? And I think that kind of thing of using, you know, maybe it's like a stock price and you want to say hey, what's Nvidia at today? Stuff like that.\n",
      "Speaker B (Erica): How about human and loop is also something that we maybe should have touched on in the eval and observability section. Because I do think, you know, I think it was like auto GPT like in 2023 of like March, people were like eh, I don't really want to let like give you access to my laptop. Like that's kind of weird. So I do think there needs to be some human in the loop kind of intervention where you can like, like no, you're taking it too far. You're generating too many emails based off of my history or something, I don't know. But I think it's important to be able to, like, cut it, like, you know, take out the cord for some of these systems and bring the autonomy back a little bit and have the human lead rather than the agent.\n",
      "Speaker A (Connor): Yeah, I really like that. I think that kind of. Well, that's what I. That's what I like about the kind of, like, generative feedback loops. I always try to, like, make sense of what the loops abstraction part of it means. And I think there's something to the AI generate some data, you give some human feedback, and then that's kind of the loop. Or of course, it's just this loop of like, the database is always providing the data to the LLM generative model to then produce the thing that it then saves back and then will be used in the future. Or the loop is like, this is how AI models are created, is the kind of like, data in AI. And I did have one question. Oh, interject, interject. Nice word choice.\n",
      "Speaker B (Erica): This comes back to the. The tree of thought conversation. And what if we. Like, as you're making. As you're logging the traces that are being observed in like, Phoenix or any of these tools or frameworks, what if the. You use the stored outputs to further, like, split the trees and then go from there? Like, maybe you're not always starting from zero. You could kind of like, store the past.\n",
      "Speaker A (Connor): Yep.\n",
      "Speaker B (Erica): I don't know. The thought isn't. It's not fully there, but kind of.\n",
      "Speaker A (Connor): Yeah. I think what you're getting at is this is kind of an interesting thing about a compound system or like an agent that's completing some task that's like, so complicated. It needs to save the intermediate parts, like, just in order to finish doing this thing. Like if you say, write a research paper or write a rebuttal to. Let me speak freely last podcast. And it's going to need to write code, save that code back into, say it uses its code in the database to manage the experiments and it runs the experiment, saves those back into this thing. And so the agent, which is the implementation of the generative feedback loop, needs to use generative feedback loops to do the thing. Yeah, exciting. So that really brings the loops part into it. Like, it can't just be like, yeah, like it. Okay, I love this. Generative feedback loops is more than just like, save the data with the LLM. It's like more of this. Yeah, it's pretty cool.\n",
      "Speaker B (Erica): It's probably like just the start of further developing a system. Like, it starts with the general feedback loop is like kind of like the start to building a more complex kind.\n",
      "Speaker A (Connor): Of pipeline, that kind of AI native business where, like your businesses, you do an AI model for some particular thing. The generative feedback loops can produce the training data to get you kicked off with such a model. Now you have the training data to get that done and get that started. And.\n",
      "Speaker B (Erica): Yeah.\n",
      "Speaker A (Connor): Nice. Very cool. Awesome. So Erica will be in New York City next Tuesday. And. Yeah, awesome. Talk about Agentic Rag. Thank you so much for joining the podcast.\n",
      "Speaker B (Erica): Thank you for having me.\n"
     ]
    }
   ],
   "source": [
    "for utterance in transcript.utterances:\n",
    "    print(f'{utterance.speaker}: {utterance.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply Sentiment Analysis, and calculate and print the number of POSITIVE, NEUTRAL, and NEGATIVE sentiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transcript contains 75 positive, 269 neutral, and 16 negative utterances.\n"
     ]
    }
   ],
   "source": [
    "n_positive, n_neutral, n_negative = 0, 0, 0\n",
    "\n",
    "for sentiment_result in transcript.sentiment_analysis:\n",
    "    if sentiment_result.sentiment == 'POSITIVE':\n",
    "       n_positive += 1\n",
    "    elif sentiment_result.sentiment == 'NEUTRAL':\n",
    "       n_neutral += 1\n",
    "    elif sentiment_result.sentiment == 'NEGATIVE':\n",
    "       n_negative += 1\n",
    "    else:\n",
    "       next\n",
    "\n",
    "print(f'The transcript contains {n_positive} positive, {n_neutral} neutral, and {n_negative} negative utterances.')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Use an LLM to summarize and answer a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a brief, bullet-point summary of the transcript for a layperson:\n",
      "\n",
      "• The podcast discusses \"Agentic RAG\" (Retrieval-Augmented Generation), which is an advanced AI system.\n",
      "\n",
      "• Agents in AI are systems that can use language models, memory, planning, and tools to complete tasks.\n",
      "\n",
      "• Agentic RAG differs from regular RAG by being able to plan steps, use multiple tools, and search different sources of information.\n",
      "\n",
      "• The speakers discuss various aspects of AI agents, including:\n",
      "  - Planning and reasoning capabilities\n",
      "  - Multi-agent systems where different AI agents work together\n",
      "  - Memory management in AI systems\n",
      "  - Challenges in evaluating AI performance\n",
      "\n",
      "• They explore how Weaviate (a company) is using AI agents in their work, particularly with \"generative feedback loops.\"\n",
      "\n",
      "• The conversation touches on the future of AI development, including:\n",
      "  - Long-running AI processes for complex tasks\n",
      "  - The need for human oversight in AI systems\n",
      "  - Potential improvements in AI tools and user interfaces\n",
      "\n",
      "• The speakers mention an upcoming event in New York City where they'll be discussing these topics further.\n"
     ]
    }
   ],
   "source": [
    "# Summarize with LLM\n",
    "\n",
    "prompt = \"Please provide a short, bullet-point summary of the transcript for a layperson.\"\n",
    "\n",
    "result = transcript.lemur.task(\n",
    "    prompt, final_model=aai.LemurModel.claude3_5_sonnet\n",
    ")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the transcript, agentic RAG (Retrieval-Augmented Generation) differs from vanilla RAG in the following key ways:\n",
      "\n",
      "1. Planning ability: Agentic RAG can plan the required steps needed to achieve the user's query, breaking down complex questions into sub-questions.\n",
      "\n",
      "2. Multiple tool usage: It can call and utilize multiple tools or data sources (e.g., database queries, web searches, APIs) to gather information, rather than just retrieving from a single source.\n",
      "\n",
      "3. Iterative searching: Agentic RAG can determine if it needs more information and retrieve from additional sources iteratively.\n",
      "\n",
      "4. Advanced querying: It can navigate databases more effectively, using vector search, aggregations, and filters based on the query requirements.\n",
      "\n",
      "5. Autonomy: Agentic RAG has more autonomy to query different databases and sources of information as needed.\n",
      "\n",
      "6. Reasoning: It can use chain-of-thought reasoning or other planning frameworks (like REACT) to better understand and address the user's query.\n",
      "\n",
      "7. Summarization: After gathering information from multiple sources, it can summarize the results if needed.\n",
      "\n",
      "In essence, agentic RAG is a more flexible, autonomous, and capable system that can handle more complex queries by leveraging multiple tools and data sources, planning its approach, and reasoning about the best way to answer a user's question.\n"
     ]
    }
   ],
   "source": [
    "# Answer a question\n",
    "\n",
    "prompt_question = \"Based on the transcript, what is agentic RAG?\"\n",
    "\n",
    "prompt_question_result = transcript.lemur.task(\n",
    "    prompt_question, final_model=aai.LemurModel.claude3_5_sonnet\n",
    ")\n",
    "\n",
    "print(prompt_question_result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
